---
title: "MAGIC AI Workshop 03 Natural Language Processing"
author: "Simon Brewer"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(888)
```

## Introduction

In this lab, we will introduce tools for natural language processing (NLP), from basic data preparation through to some exploration and building a simple machine learning model. We are only scratching the surface of what is possible with NLP methods in this lab. See the tidytext website for further examples. 

You'll need several packages for the lab including:

- `tidytext`: a library for cleaning and processing text data
- ?? SOME OTHER ONES

```{r eval=FALSE}
install.packages(c("tidytext"))
```

Now load the first few packages:

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
```

## Data

We'll use a set of tweets related to climate change from 2015 to 2018, taken from:

https://www.kaggle.com/datasets/edqian/twitter-climate-change-sentiment-dataset

The data are held in the file *twitter_sentiment_data.csv*, which you can download from the github repository. Read these in and take a quick look. There are three columns: a sentiment estimate, the tweet (`message`) and a tweet id. The sentiment estimate was provided by a group of experts and are tagged as follows:

- `2` (News): the tweet links to factual news about climate change
- `1` (Pro): the tweet supports the belief of man-made climate change
- `0` (Neutral): the tweet neither supports nor refutes the belief of man-made climate change
- `-1`(Anti): the tweet does not believe in man-made climate change

```{r}
dat <- read.csv("../datafiles/twitter_sentiment_data.csv")
head(dat)
```

Our basic plan here is:

1. Prepare the data for analysis. 
2. Visualize and explore the data
3. Create an embedding for the tweets. This represents each tweet as a vector of numbers, and can be used for further analysis
4. Create a simple machine learning model to predict the sentiment of a tweet

## Text processing

Processing text data into a usable form can be one of the most time consuming parts of the analysis. Basically, we want to remove any characters or words that are irrelevant to any analysis. In addition, we should try to simplify and standardize the language used. For example, a computer will not necessarily recognize that 'see' and 'seen' are related to each other. 


### General cleaning

First, we'll remove any retweets from the dataset (indicated by `RT` at the start of the message). While there are some applications where the number of retweets are of interest, we will consider them as duplicates for this exercise. 

```{r}
dat = dat %>%
  filter(str_starts(message, "RT", negate = TRUE))
```

To illustrate the next steps, we'll extract the fourth tweet from the dataset:

```{r}
tweet = dat[4, ]
print(tweet$message)
```

This is a typical tweet and has several issues for text processing:

- There is a URL at the end of the tweet
- There is at least one username (`@...`)
- There are several hashtag (`#...`)

We'll use several steps to clean this up. To illustrate these, we'll walk through the individual steps for the first 5 tweets.

- First extract the first 5 tweets:

```{r}
tidy_dat <- dat %>%
  slice_head(n = 5) 
```

- Remove various non-words (URLs, symbols, etc)

```{r}
tidy_dat <- tidy_dat %>%
  mutate(message = str_replace_all(message, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https", "")) 
head(tidy_dat)
```

- Remove usernames (starting with `@..`)

```{r}
tidy_dat <- tidy_dat %>% 
  mutate(message = str_replace_all(message, "@\\w+", "")) 
head(tidy_dat)
```

- Convert the tweets into individual words or tokens. Note that this converts the data from being one line per tweet to one line per work

```{r}
tidy_dat <- tidy_dat %>%
  unnest_tokens(word, message)
head(tidy_dat)
```

- Finally, remove stopwords. These are a predefined set of commonly occurring words that have little value in analysis (e.g. the, and, ...). 

```{r}
tidy_dat <- tidy_dat %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))
head(tidy_dat)
```

### Word matching

The last thing we'll need to do is match words with similar meanings. There's a couple of approaches to this: stemming and lemmatization. Stemming strips words back to the core stem using `wordStem()` from the SmowballC library. For example, here are 5 different words related to programming. The stemmer converts them all to `program`:

```{r}
library(SnowballC)
words <- c("program","programming","programer","programs","programmed")
wordStem(words)
```

One disadvantage to this is that the stems may no longer reflect actual words. For example, the stem to climate is `climat`:

```{r}
wordStem("climate")
```

The second issue is that stemming does not account for context - that words with different meanings may be spelled the same, and can only be distinguished in the context of the sentence. 

Lemmatization attempts to avoid these issues by converting words to a standard form, and accounting for the meaning of the surrounding words. Here we'll use the **spacyr** package to perform lemmatization. This runs a python package in the packground and needs to be set up the first time you use it with `spacy_install()` 

```{r message=FALSE, warning=FALSE}
library(spacyr)
## spacy_install() ## Run first time after installation
spacy_initialize(entity = FALSE)
```
Now compare the conversion of `saw` in these two phrases:

```{r message=FALSE, warning=FALSE}
spacy_parse("Owen saw a rabbit")
```

```{r message=FALSE, warning=FALSE}
spacy_parse("Owen cut a plank with a saw")
```


